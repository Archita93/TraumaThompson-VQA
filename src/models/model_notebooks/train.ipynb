{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary Testing - Phase 4 - Model Training - Multi-class classification\n",
    "\n",
    "This notebook attempts to implement the training script general to all the models - CLIP, BLIP, and ViLT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    CLIPModel, CLIPProcessor,\n",
    "    BlipForQuestionAnswering, BlipProcessor,\n",
    "    ViltForQuestionAnswering, ViltProcessor\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions and answer classes\n",
    "questions = [\n",
    "    \"What limb is injured?\",\n",
    "    \"Is the patient intubated?\",\n",
    "    \"Where is the catheter inserted?\",\n",
    "    \"Is there bleeding?\",\n",
    "    \"Has the bleeding stopped?\",\n",
    "    \"Is the patient moving?\",\n",
    "    \"Is the patient breathing?\",\n",
    "    \"Is there a tourniquet?\",\n",
    "    \"Is there a chest tube?\",\n",
    "    \"Are the patient and instruments secured?\",\n",
    "    \"If a limb is missing which one?\",\n",
    "    \"Is there mechanical ventilation?\",\n",
    "    \"What is the position of the injury?\"\n",
    "]\n",
    "\n",
    "classes = [\n",
    "    ['no limb is injured', 'left leg', 'left arm', 'right leg', 'right arm'],\n",
    "    [\"can't identify\", 'no', 'yes'],\n",
    "    ['no catheter is used', 'lower limb'],\n",
    "    ['no', 'yes'],\n",
    "    ['there is no bleeding', 'no', 'yes'],\n",
    "    [\"can't identify\", 'yes', 'no'],\n",
    "    [\"can't identify\", 'no', 'yes'],\n",
    "    ['no', 'yes'],\n",
    "    ['no', 'yes'],\n",
    "    ['no', 'yes', \"can't identify\"],\n",
    "    ['none', 'left arm', 'left leg', 'right leg'],\n",
    "    [\"can't identify\", 'no', 'yes'],\n",
    "    ['thorax', 'throat', \"can't identify\", 'lower limb', 'abdomen', 'upper limb']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Dataset for classification\n",
    "class ClassificationVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor, classes):\n",
    "        self.data = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.qa_columns = dataframe.columns[2:]\n",
    "        self.label_encoders = [LabelEncoder().fit(cls) for cls in classes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * len(self.qa_columns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_idx = idx // len(self.qa_columns)\n",
    "        q_idx = idx % len(self.qa_columns)\n",
    "        row = self.data.iloc[row_idx]\n",
    "        question = self.qa_columns[q_idx]\n",
    "        answer = row[question]\n",
    "        if pd.isna(answer):\n",
    "            next_idx = (idx + 1) % len(self)\n",
    "            return self.__getitem__(next_idx)\n",
    "        image_path = os.path.join(self.image_dir, row['video_id'], f\"{row['video_id']}_frame{row['frame']}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.label_encoders[q_idx].transform([str(answer).strip()])[0]\n",
    "        return {\n",
    "            \"text\": question.strip(),\n",
    "            \"image\": image,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"question_idx\": q_idx\n",
    "        }\n",
    "\n",
    "# Collate function generator\n",
    "def get_collate_fn(processor):\n",
    "    def classification_collate(batch):\n",
    "        texts = [item[\"text\"] for item in batch]\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        labels = torch.stack([item[\"label\"] for item in batch])\n",
    "        question_idxs = [item[\"question_idx\"] for item in batch]\n",
    "        processed = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        return {\n",
    "            \"input_ids\": processed[\"input_ids\"],\n",
    "            \"attention_mask\": processed[\"attention_mask\"],\n",
    "            \"pixel_values\": processed[\"pixel_values\"],\n",
    "            \"labels\": labels,\n",
    "            \"question_idxs\": question_idxs\n",
    "        }\n",
    "    return classification_collate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified classifier\n",
    "class VQAClassifier(nn.Module):\n",
    "    def __init__(self, model_name, base_model, hidden_dim, num_classes_per_question):\n",
    "        super().__init__()\n",
    "        self.name = model_name\n",
    "        self.base = base_model\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, num_classes) for num_classes in num_classes_per_question\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, question_idx):\n",
    "        if self.name == \"blip\":\n",
    "            # BLIP-specific pooling from text encoder\n",
    "            vision_outputs = self.base.vision_model(pixel_values=pixel_values)\n",
    "            image_embeds = vision_outputs.last_hidden_state\n",
    "\n",
    "            text_inputs = self.base.text_encoder.embeddings(input_ids=input_ids)\n",
    "            text_outputs = self.base.text_encoder.encoder(\n",
    "                hidden_states=text_inputs,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            pooled = text_outputs.last_hidden_state[:, 0, :]  # [CLS]\n",
    "        \n",
    "        elif self.name == \"clip\":\n",
    "            outputs = self.base(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "            pooled = outputs.text_embeds + outputs.image_embeds\n",
    "\n",
    "        else:  # For ViLT and any other HuggingFace encoder-style model\n",
    "            outputs = self.base(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "            pooled = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        logits = self.classifiers[question_idx](pooled)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViltModel\n",
    "\n",
    "# Model registry\n",
    "models = {\n",
    "    \"clip\": {\n",
    "        \"model\": CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
    "        \"processor\": CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\"),\n",
    "        \"hidden_dim\": 512\n",
    "    },\n",
    "    \"blip\": {\n",
    "        \"model\": BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\"),\n",
    "        \"processor\": BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\"),\n",
    "        \"hidden_dim\": 768\n",
    "    },\n",
    "    \"vilt\": {\n",
    "        \"model\": ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\"),\n",
    "        \"processor\": ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\"),\n",
    "        \"hidden_dim\": 768\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"data/sample-df.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: clip, blip, vilt\n"
     ]
    }
   ],
   "source": [
    "# Prompt user for model selection\n",
    "print(\"Available models:\", \", \".join(models.keys()))\n",
    "selected_model = input(\"Enter the model you want to train (clip, blip, vilt): \").strip().lower()\n",
    "\n",
    "if selected_model not in models:\n",
    "    raise ValueError(f\"Invalid model name. Choose from: {list(models.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training VILT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - vilt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:13<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VILT Epoch 1 Avg Loss: 1.0944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - vilt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:14<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VILT Epoch 2 Avg Loss: 0.8698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - vilt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:16<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VILT Epoch 3 Avg Loss: 0.6163\n"
     ]
    }
   ],
   "source": [
    "# Train selected model\n",
    "config = models[selected_model]\n",
    "print(f\"\\nðŸš€ Training {selected_model.upper()}...\")\n",
    "processor = config[\"processor\"]\n",
    "collate_fn = get_collate_fn(processor)\n",
    "train_dataset = ClassificationVQADataset(train_df, image_dir=\"frames_sample\", processor=processor, classes=classes)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "base_model = config[\"model\"].to(device)\n",
    "model = VQAClassifier(selected_model, base_model, config[\"hidden_dim\"], [len(c) for c in classes]).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - {selected_model}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        question_idxs = batch[\"question_idxs\"]\n",
    "\n",
    "        losses = []\n",
    "        for i in range(len(input_ids)):\n",
    "            logits = model(\n",
    "                input_ids[i].unsqueeze(0),\n",
    "                attention_mask[i].unsqueeze(0),\n",
    "                pixel_values[i].unsqueeze(0),\n",
    "                question_idx=question_idxs[i]\n",
    "            )\n",
    "            loss = criterion(logits, labels[i].unsqueeze(0))\n",
    "            losses.append(loss)\n",
    "\n",
    "        batch_loss = sum(losses) / len(losses)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"âœ… {selected_model.upper()} Epoch {epoch+1} Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), f\"pt/{selected_model}_classifier.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary Testing - Phase 3 - Model Training - Multi-class classification\n",
    "\n",
    "This notebook attempts to implement the training and evaluation script specific to CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archita/Library/CloudStorage/OneDrive-SimonFraserUniversity(1sfu)/Track 2/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Sample Questions and Classes\n",
    "questions = [\n",
    "    \"What limb is injured?\",\n",
    "    \"Is the patient intubated?\",\n",
    "    \"Where is the catheter inserted?\",\n",
    "    \"Is there bleeding?\",\n",
    "    \"Has the bleeding stopped?\",\n",
    "    \"Is the patient moving?\",\n",
    "    \"Is the patient breathing?\",\n",
    "    \"Is there a tourniquet?\",\n",
    "    \"Is there a chest tube?\",\n",
    "    \"Are the patient and instruments secured?\",\n",
    "    \"If a limb is missing which one?\",\n",
    "    \"Is there mechanical ventilation?\",\n",
    "    \"What is the position of the injury?\"\n",
    "]\n",
    "\n",
    "classes = [\n",
    "    ['no limb is injured', 'left leg', 'left arm', 'right leg', 'right arm'],\n",
    "    [\"can't identify\", 'no', 'yes'],\n",
    "    ['no catheter is used', 'lower limb'],\n",
    "    ['no', 'yes'],\n",
    "    ['there is no bleeding', 'no', 'yes'],\n",
    "    [\"can't identify\", 'yes', 'no'],\n",
    "    [\"can't identify\", 'no', 'yes'],\n",
    "    ['no', 'yes'],\n",
    "    ['no', 'yes'],\n",
    "    ['no', 'yes', \"can't identify\"],\n",
    "    ['none', 'left arm', 'left leg', 'right leg'],\n",
    "    [\"can't identify\", 'no', 'yes'],\n",
    "    ['thorax', 'throat', \"can't identify\", 'lower limb', 'abdomen', 'upper limb']\n",
    "]\n",
    "\n",
    "# Load CLIP processor\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class ClassificationVQADataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, processor, classes):\n",
    "        self.data = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.qa_columns = dataframe.columns[2:]\n",
    "        self.label_encoders = [LabelEncoder().fit(cls) for cls in classes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * len(self.qa_columns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_idx = idx // len(self.qa_columns)\n",
    "        q_idx = idx % len(self.qa_columns)\n",
    "        row = self.data.iloc[row_idx]\n",
    "        question = self.qa_columns[q_idx]\n",
    "        answer = row[question]\n",
    "        if pd.isna(answer):\n",
    "            next_idx = (idx + 1) % len(self)\n",
    "            return self.__getitem__(next_idx)\n",
    "        image_path = os.path.join(self.image_dir, row['video_id'], f\"{row['video_id']}_frame{row['frame']}.jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = self.label_encoders[q_idx].transform([str(answer).strip()])[0]\n",
    "        return {\n",
    "            \"text\": question.strip(),\n",
    "            \"image\": image,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"question_idx\": q_idx\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function for batching\n",
    "def classification_collate(batch):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    labels = torch.stack([item[\"label\"] for item in batch])\n",
    "    question_idxs = [item[\"question_idx\"] for item in batch]\n",
    "    processed = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return {\n",
    "        \"input_ids\": processed[\"input_ids\"],\n",
    "        \"attention_mask\": processed[\"attention_mask\"],\n",
    "        \"pixel_values\": processed[\"pixel_values\"],\n",
    "        \"labels\": labels,\n",
    "        \"question_idxs\": question_idxs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP-based multi-class classifier\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, num_classes_per_question):\n",
    "        super().__init__()\n",
    "        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.classifiers = nn.ModuleList([\n",
    "            nn.Linear(self.clip.config.projection_dim, num_classes) for num_classes in num_classes_per_question\n",
    "        ])\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pixel_values, question_idx):\n",
    "        outputs = self.clip(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
    "        pooled_output = outputs.text_embeds + outputs.image_embeds\n",
    "        logits = self.classifiers[question_idx](pooled_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "train_df = pd.read_csv(\"data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"data/test_data.csv\")\n",
    "\n",
    "# Build dataset and dataloader\n",
    "train_dataset = ClassificationVQADataset(train_df, image_dir=\"frames\", processor=processor, classes=classes)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=classification_collate)\n",
    "\n",
    "# Initialize model\n",
    "model = CLIPClassifier([len(c) for c in classes]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 5191/5191 [1:40:44<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 Average Loss: 0.6270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 5191/5191 [2:48:41<00:00,  1.95s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 Average Loss: 0.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 5191/5191 [2:30:09<00:00,  1.74s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 Average Loss: 0.3977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 5191/5191 [2:30:03<00:00,  1.73s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 Average Loss: 0.3429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 5191/5191 [2:27:25<00:00,  1.70s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 Average Loss: 0.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 5191/5191 [2:22:53<00:00,  1.65s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 Average Loss: 0.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 5191/5191 [1:27:50<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 Average Loss: 0.2494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 5191/5191 [1:10:44<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 Average Loss: 0.2261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(8):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        question_idxs = batch[\"question_idxs\"]\n",
    "\n",
    "        losses = []\n",
    "        for i in range(len(input_ids)):\n",
    "            logits = model(\n",
    "                input_ids[i].unsqueeze(0),\n",
    "                attention_mask[i].unsqueeze(0),\n",
    "                pixel_values[i].unsqueeze(0),\n",
    "                question_idx=question_idxs[i]\n",
    "            )\n",
    "            loss = criterion(logits, labels[i].unsqueeze(0))\n",
    "            losses.append(loss)\n",
    "\n",
    "        batch_loss = sum(losses) / len(losses)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += batch_loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"clip_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load processor and model\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPClassifier([len(cls) for cls in classes])\n",
    "model.load_state_dict(torch.load(\"clip_classifier.pt\"))  # <-- load saved model\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load data\n",
    "# _, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create test dataset & loader\n",
    "test_dataset = ClassificationVQADataset(test_df, image_dir=\"frames_sample\", processor=processor, classes=classes)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=classification_collate)\n",
    "\n",
    "# Set up label encoders (must match training!)\n",
    "label_encoders = [LabelEncoder().fit(cls) for cls in classes]\n",
    "\n",
    "# Initialize results tracker\n",
    "all_preds = [[] for _ in questions]\n",
    "all_truths = [[] for _ in questions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  video_id  frame                         question           true_answer  \\\n",
      "0   Cric20      1            What limb is injured?    no limb is injured   \n",
      "1   Cric20      1        Is the patient intubated?        can't identify   \n",
      "2   Cric20      1  Where is the catheter inserted?   no catheter is used   \n",
      "3   Cric20      1               Is there bleeding?                    no   \n",
      "4   Cric20      1        Has the bleeding stopped?  there is no bleeding   \n",
      "\n",
      "       predicted_answer  \n",
      "0    no limb is injured  \n",
      "1        can't identify  \n",
      "2   no catheter is used  \n",
      "3                    no  \n",
      "4  there is no bleeding  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        question_idxs = batch[\"question_idxs\"]\n",
    "\n",
    "        for i in range(len(input_ids)):\n",
    "            q_idx = question_idxs[i]\n",
    "\n",
    "            logits = model(\n",
    "                input_ids[i].unsqueeze(0),\n",
    "                attention_mask[i].unsqueeze(0),\n",
    "                pixel_values[i].unsqueeze(0),\n",
    "                question_idx=q_idx\n",
    "            )\n",
    "\n",
    "            pred_idx = logits.argmax(dim=1).item()\n",
    "            pred_label = label_encoders[q_idx].inverse_transform([pred_idx])[0]\n",
    "            true_label = label_encoders[q_idx].inverse_transform([labels[i].item()])[0]\n",
    "\n",
    "            # Get original row in DataFrame\n",
    "            dataset_idx = batch_idx * test_loader.batch_size + i\n",
    "            frame_row_idx = dataset_idx // len(questions)  # one row generates len(questions) samples\n",
    "            question = questions[q_idx]\n",
    "\n",
    "            video_id = test_dataset.data.iloc[frame_row_idx][\"video_id\"]\n",
    "            frame = test_dataset.data.iloc[frame_row_idx][\"frame\"]\n",
    "\n",
    "            results.append({\n",
    "                \"video_id\": video_id,\n",
    "                \"frame\": frame,\n",
    "                \"question\": question,\n",
    "                \"true_answer\": true_label,\n",
    "                \"predicted_answer\": pred_label\n",
    "            })\n",
    "\n",
    "# Build DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"clip_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy per question:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy per question\n",
    "print(\"\\nAccuracy per question:\\n\")\n",
    "for i, q in enumerate(questions):\n",
    "    le = label_encoders[i]\n",
    "    if all_truths[i]:\n",
    "        acc = accuracy_score(all_truths[i], all_preds[i])\n",
    "        print(f\"{q}: {acc:.2%}\")\n",
    "        # Show first 3 predictions\n",
    "        for j in range(min(3, len(all_preds[i]))):\n",
    "            pred_label = le.inverse_transform([all_preds[i][j]])[0]\n",
    "            true_label = le.inverse_transform([all_truths[i][j]])[0]\n",
    "            print(f\"Pred: {pred_label} | GT: {true_label}\")\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
